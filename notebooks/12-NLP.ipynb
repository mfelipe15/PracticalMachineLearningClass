{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Natural Language Processing\n",
    "\n",
    "by [Alejandro Correa Bahnsen](albahnsen.com/)\n",
    "\n",
    "version 0.1, Apr 2016\n",
    "\n",
    "## Part of the class [Practical Machine Learning](https://github.com/albahnsen/PracticalMachineLearningClass)\n",
    "\n",
    "\n",
    "\n",
    "This notebook is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](http://creativecommons.org/licenses/by-sa/3.0/deed.en_US). Special thanks goes to [Kevin Markham](https://github.com/justmarkham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language\n",
    "\n",
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [My application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)\n",
    "    \n",
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**\n",
    "\n",
    "### Why is NLP hard?\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Reading text from Mashable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = ['http://mashable.com/2016/03/07/apple-ebook-case/#6KXWVluVqmqg',\n",
    "        'http://mashable.com/2016/03/31/donald-trump-gaslighting-women',\n",
    "        'http://mashable.com/2016/03/08/scotland-giant-rabbit-home',\n",
    "        'http://mashable.com/2016/03/08/15-skills-digital-marketers',\n",
    "        'http://mashable.com/2015/12/31/top-ten-movies-2015/#BMwhrISr3sqB',\n",
    "        'http://mashable.com/2016/01/01/munich-terror-attack-warning/#POjevJ.v3OqD',\n",
    "        'http://mashable.com/2016/01/01/natalie-cole-death/#et7PagRa3Zqh',\n",
    "        'http://mashable.com/2016/01/01/camille-cosby-will-testify/#SGpnofBo3aqd',\n",
    "        'http://mashable.com/2016/01/31/obama-first-visit-mosque/',\n",
    "        'http://mashable.com/2016/01/31/australian-open-pics-decade/',\n",
    "        'http://mashable.com/2016/01/31/k9-auction-fundraiser/',\n",
    "        'http://mashable.com/2016/03/14/apple-vs-fbi-whatsapp-encryption/',\n",
    "        'http://mashable.com/2016/03/10/apple-battery-life/',\n",
    "        'http://mashable.com/2016/03/07/iphone-photo-editing-tips/',\n",
    "        'http://mashable.com/2015/11/30/just-cause-3-review/',\n",
    "        'http://mashable.com/2015/11/30/paris-holiday/',\n",
    "        'http://mashable.com/2015/11/30/australia-fossil-fuel-pledge/',\n",
    "        'http://mashable.com/2015/11/30/paris-holiday',\n",
    "        'http://mashable.com/2015/12/01/samsung-koh-dongjin/',\n",
    "        'http://mashable.com/2016/04/05/jackie-chan-kung-fu-yoga-bollywood/',\n",
    "        'http://mashable.com/2015/12/01/asian-7-eleven-food/',\n",
    "        'http://mashable.com/2015/12/31/5-apps-hangover-fixes/',\n",
    "        'http://mashable.com/2015/12/31/latest-hillary-clinton-email-dump/',\n",
    "        'http://mashable.com/2015/12/14/pu-zhiqiang-trial/',\n",
    "        'http://mashable.com/2015/12/14/oukitel-k10000-15-day-battery/',\n",
    "        'http://mashable.com/2016/01/13/donald-trump-mic/',\n",
    "        'http://mashable.com/2016/01/14/nissan-salutes-dodge-chevy-ford-ad/',\n",
    "        'http://mashable.com/2016/01/31/boko-haram-burned-children-nigeria/',\n",
    "        'http://mashable.com/2016/01/31/louis-ck-show/',\n",
    "        'http://mashable.com/2016/01/31/disney-princess-tim-burton/',\n",
    "        'http://mashable.com/2016/02/01/tata-zica-zika-virus/',\n",
    "        'http://mashable.com/2016/02/01/guess-skin-hydrating-denim/',\n",
    "        'http://mashable.com/2016/02/01/human-gene-editing/',\n",
    "        'http://mashable.com/2016/02/01/donald-trump-loser/',\n",
    "        'http://mashable.com/2016/02/02/jaguar-land-rover-self-driving/',\n",
    "        'http://mashable.com/2016/02/02/liam-neeson-super-bowl/',\n",
    "        'http://mashable.com/2016/02/02/how-to-grieve-at-work/',\n",
    "        'http://mashable.com/2016/02/02/donald-trump-puffer-fish/',\n",
    "        'http://mashable.com/2016/02/02/virustotal-firmware-scanner/',\n",
    "        'http://mashable.com/2016/02/02/sanders-supporters-conspiracy-theories/',\n",
    "        'http://mashable.com/2016/02/02/somalia-plane-hole-in-side/',\n",
    "        'http://mashable.com/2016/02/02/sleeping-cat-covered-in-mice/',\n",
    "        'http://mashable.com/2016/02/02/suicide-squad-batman-animated-series/',\n",
    "        'http://mashable.com/2016/02/03/super-bowl-kids/',\n",
    "        'http://mashable.com/2016/02/11/master-of-none-season-2/',\n",
    "        'http://mashable.com/2016/02/11/kanye-album-looks-like/',\n",
    "        'http://mashable.com/2016/02/11/bernie-sanders-erica-garner/',\n",
    "        'http://mashable.com/2016/02/11/porter-ranch-gas-leak-plugged/',\n",
    "        'http://mashable.com/2016/02/12/milestone-black-superhero/',\n",
    "        'http://mashable.com/2016/02/12/ohio-arts-sells-etch-a-sketch/',\n",
    "        'http://mashable.com/2016/02/25/sarah-michelle-gellar-cruel-intentions-pilot/',\n",
    "        'http://mashable.com/2016/02/25/new-york-hoverboard-ban-posters/',\n",
    "        'http://mashable.com/2015/09/30/australia-satellite-rural-internet/',\n",
    "        'http://mashable.com/2015/09/30/code-black-doctor-training/',\n",
    "        'http://mashable.com/2015/10/01/tribeca-shortlist-netflix-good-movies/',\n",
    "        'http://mashable.com/2015/10/01/charles-ingram-evicted-a9/',\n",
    "        'http://mashable.com/2015/10/04/startups-for-mbas/',\n",
    "        'http://mashable.com/2015/10/04/wheeliz-car-sharing-disabilities/',\n",
    "        'http://mashable.com/2015/10/04/bird-throws-coins/',\n",
    "        'http://mashable.com/2015/10/04/columbia-flood-instagram/',\n",
    "        'http://mashable.com/2015/10/04/good-wife-season-7-premiere-recap/',\n",
    "        'http://mashable.com/2015/10/04/last-man-on-earth-will-ferrell/',\n",
    "        'http://mashable.com/2015/10/15/hotel-adult-movies/',\n",
    "        'http://mashable.com/2015/10/15/hillary-clinton-wine-ice-cream/',\n",
    "        'http://mashable.com/2015/10/15/pc-ipad-samsung-iphone-microsoft/',\n",
    "        'http://mashable.com/2015/10/15/103-year-old-birthday-wonder-woman/',\n",
    "        'http://mashable.com/2015/10/15/back-to-the-future-2015-short/',\n",
    "        'http://mashable.com/2015/10/16/emoji-movie-trailer-the-soup/',\n",
    "        'http://mashable.com/2015/10/16/lego-batman-rosario-dawson/',\n",
    "        'http://mashable.com/2015/10/16/ford-flux-capacitor/',\n",
    "        'http://mashable.com/2015/10/28/turkey-media-blackout-journalists/',\n",
    "        'http://mashable.com/2015/10/28/robert-downey-jr-cystic-fibrosis/',\n",
    "        'http://mashable.com/2015/10/28/country-life-gentleman/',\n",
    "        'http://mashable.com/2015/10/28/emojis-youtube-videos/',\n",
    "        'http://mashable.com/2015/08/31/jarryd-hayne-49ers-roster/',\n",
    "        'http://mashable.com/2015/09/01/concept-fuci-smartbike/',\n",
    "        'http://mashable.com/2015/09/01/american-horror-story-hotel-teasers/',\n",
    "        'http://mashable.com/2015/09/17/brabus-zero-emission-tesla/',\n",
    "        'http://mashable.com/2015/09/17/refugees-croatia-thousands/',\n",
    "        'http://mashable.com/2015/09/17/reputation-meltdown-office/',\n",
    "        'http://mashable.com/2015/09/17/google-glass-project-aura/',\n",
    "        'http://mashable.com/2015/09/17/apple-steve-wozniak-ahmed-mohamed/',\n",
    "        'http://mashable.com/2015/09/30/gap-pace-brandspeak/',\n",
    "        'http://mashable.com/2015/09/30/private-jets-to-cuba/',\n",
    "        'http://mashable.com/2015/09/30/chinas-glass-bottomed-suspension-bridge/'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract text and other info (From previous class)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def news_info(url):\n",
    "    # Download HTML\n",
    "    response = urllib.request.urlopen(url)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Title, author, text\n",
    "    title = soup.title.string\n",
    "    \n",
    "    author = soup.find_all(\"span\", { \"class\" : \"author_name\"})\n",
    "    # If author is empty try this:\n",
    "    if author == []:\n",
    "        author = soup.find_all(\"span\", { \"class\" : \"byline basic\"})\n",
    "    author = str(author).split('>')[1].split('By ')[1].split('<')[0]\n",
    "    \n",
    "    # Number of shares\n",
    "    shares = soup.find_all(\"div\", { \"class\" : \"total-shares\"})\n",
    "    try:\n",
    "        shares = str(shares).split('<em>')[1].split('</em>')[0]\n",
    "    except IndexError:\n",
    "        shares = str(shares).split('<em class=\"minimal\">')[1].split('</em>')[0]\n",
    "\n",
    "    if 'k' in shares:\n",
    "        shares = shares[:-1]\n",
    "        shares = shares.replace('.', '') + '00'\n",
    "    \n",
    "    # Get text\n",
    "    try:\n",
    "        text = str(soup.get_text()).split(\"UTC\\n\\n\\n\")[1]\n",
    "    except IndexError:\n",
    "        text = str(soup.get_text()).split(\"Analysis\\n\\n\")[1]\n",
    "        \n",
    "    text = text.split('Have something to add to this story?')[0]\n",
    "    \n",
    "    author_web = soup.find_all(\"a\", { \"class\" : \"byline\"})\n",
    "    if author_web != []:\n",
    "        author_web = 'http://mashable.com' + str(author_web).split('href=\"')[1].split('\">')[0]\n",
    "    \n",
    "        # Author networks\n",
    "        author_networks = {'facebo': '',\n",
    "                           'linked': '',\n",
    "                           'twitte': '',\n",
    "                           'google': ''}\n",
    "\n",
    "        response = urllib.request.urlopen(author_web)\n",
    "        html = response.read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        networks = str(soup.find_all(\"div\", { \"class\" : \"profile-networks\"})).replace('network-badge-round', '').split('network-badge-')\n",
    "\n",
    "        for network in networks:\n",
    "            if network[:6] in author_networks.keys():\n",
    "                author_networks[network[:6]] = network.split('href=\"')[1].split('\" target')[0]\n",
    "\n",
    "        # Author twitter followers\n",
    "        author_networks['twitter_followers'] = 0\n",
    "        if author_networks['twitte'] != '':\n",
    "\n",
    "            response = urllib.request.urlopen(author_networks['twitte'])\n",
    "            html = response.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            followers = str(soup.find_all(\"span\", { \"class\" : \"ProfileNav-value\"})[2]).split('\">')[1]\n",
    "            if ('K' in followers) or ('mil' in followers):\n",
    "                followers = followers.split('\\xa0')[0]\n",
    "                if ',' in followers:\n",
    "                    followers = followers.replace(',', '') + '00'\n",
    "                else:\n",
    "                    followers = followers + '000'\n",
    "            else:\n",
    "                followers = followers.split('</span')[0].replace('.', '')\n",
    "\n",
    "            author_networks['twitter_followers'] = int(followers)\n",
    "        \n",
    "    else:\n",
    "        author_networks = {'facebo': '',\n",
    "                           'linked': '',\n",
    "                           'twitte': '',\n",
    "                           'google': '', \n",
    "                           'twitter_followers': 0}\n",
    "        \n",
    "    return {'title': title, 'author': author, 'shares': shares, \n",
    "            'author_web': author_web, 'text':text, \n",
    "            'author_networks': author_networks}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    try:\n",
    "        data.append(news_info(url))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df, pd.DataFrame.from_records(df.author_networks.tolist()), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('author_networks', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('12_mashable_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_web</th>\n",
       "      <th>shares</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>facebo</th>\n",
       "      <th>google</th>\n",
       "      <th>linked</th>\n",
       "      <th>twitte</th>\n",
       "      <th>twitter_followers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seth Fiegerman</td>\n",
       "      <td>http://mashable.com/people/seth-fiegerman/</td>\n",
       "      <td>4900</td>\n",
       "      <td>\\nApple's long and controversial ebook case ha...</td>\n",
       "      <td>The Supreme Court smacked down Apple today</td>\n",
       "      <td>http://www.facebook.com/sfiegerman</td>\n",
       "      <td></td>\n",
       "      <td>http://www.linkedin.com/in/sfiegerman</td>\n",
       "      <td>https://twitter.com/sfiegerman</td>\n",
       "      <td>14300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rebecca Ruiz</td>\n",
       "      <td>http://mashable.com/people/rebecca-ruiz/</td>\n",
       "      <td>1900</td>\n",
       "      <td>Analysis\\n\\n\\n\\n\\n\\nThere is a reason that Don...</td>\n",
       "      <td>Every woman has met a man like Donald Trump</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/rebecca_ruiz</td>\n",
       "      <td>3738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Davina Merchant</td>\n",
       "      <td>http://mashable.com/people/568bdab351984019310...</td>\n",
       "      <td>7000</td>\n",
       "      <td>LONDON - Last month we reported on a dog-sized...</td>\n",
       "      <td>Adorable dog-sized rabbit finally finds his fo...</td>\n",
       "      <td></td>\n",
       "      <td>https://plus.google.com/105525238342980116477?...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scott Gerber</td>\n",
       "      <td>[]</td>\n",
       "      <td>5000</td>\n",
       "      <td>Today's digital marketing experts must have a ...</td>\n",
       "      <td>15 essential skills all digital marketing hire...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Josh Dickey</td>\n",
       "      <td>http://mashable.com/people/joshdickey/</td>\n",
       "      <td>1600</td>\n",
       "      <td>LOS ANGELES — For big, fun, populist popcorn m...</td>\n",
       "      <td>Mashable top 10: 'The Force Awakens' is the be...</td>\n",
       "      <td></td>\n",
       "      <td>https://plus.google.com/109213469090692520544?...</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/JLDlite</td>\n",
       "      <td>11200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                         author_web shares  \\\n",
       "0   Seth Fiegerman         http://mashable.com/people/seth-fiegerman/   4900   \n",
       "1     Rebecca Ruiz           http://mashable.com/people/rebecca-ruiz/   1900   \n",
       "2  Davina Merchant  http://mashable.com/people/568bdab351984019310...   7000   \n",
       "3     Scott Gerber                                                 []   5000   \n",
       "4      Josh Dickey             http://mashable.com/people/joshdickey/   1600   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\nApple's long and controversial ebook case ha...   \n",
       "1  Analysis\\n\\n\\n\\n\\n\\nThere is a reason that Don...   \n",
       "2  LONDON - Last month we reported on a dog-sized...   \n",
       "3  Today's digital marketing experts must have a ...   \n",
       "4  LOS ANGELES — For big, fun, populist popcorn m...   \n",
       "\n",
       "                                               title  \\\n",
       "0         The Supreme Court smacked down Apple today   \n",
       "1        Every woman has met a man like Donald Trump   \n",
       "2  Adorable dog-sized rabbit finally finds his fo...   \n",
       "3  15 essential skills all digital marketing hire...   \n",
       "4  Mashable top 10: 'The Force Awakens' is the be...   \n",
       "\n",
       "                               facebo  \\\n",
       "0  http://www.facebook.com/sfiegerman   \n",
       "1                                       \n",
       "2                                       \n",
       "3                                       \n",
       "4                                       \n",
       "\n",
       "                                              google  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  https://plus.google.com/105525238342980116477?...   \n",
       "3                                                      \n",
       "4  https://plus.google.com/109213469090692520544?...   \n",
       "\n",
       "                                  linked                            twitte  \\\n",
       "0  http://www.linkedin.com/in/sfiegerman    https://twitter.com/sfiegerman   \n",
       "1                                         https://twitter.com/rebecca_ruiz   \n",
       "2                                                                            \n",
       "3                                                                            \n",
       "4                                              https://twitter.com/JLDlite   \n",
       "\n",
       "   twitter_followers  \n",
       "0              14300  \n",
       "1               3738  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4              11200  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tokenization\n",
    "\n",
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "# from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('12_mashable_texts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the target feature (number of shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       82.000000\n",
       "mean      3090.487805\n",
       "std       8782.031594\n",
       "min        437.000000\n",
       "25%        893.500000\n",
       "50%       1200.000000\n",
       "75%       2275.000000\n",
       "max      63100.000000\n",
       "Name: shares, dtype: float64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.shares\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = pd.cut(y, [0, 893, 1200, 2275, 63200], labels=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    22\n",
       "3    21\n",
       "0    21\n",
       "2    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['y'] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create document-term matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X\n",
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 7969)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ydwnm50jlu', 'ye', 'yeah', 'year', 'years', 'yec', 'yeezy', 'yellow', 'yelp', 'yep', 'yes', 'yesterday', 'yesweather', 'yet', 'yoga', 'yong', 'york', 'you', 'young', 'younger', 'youngest', 'your', 'yourself', 'youth', 'youtube', 'youtubeduck', 'yup', 'yuyuan', 'yücel', 'zach', 'zaxoqbv487', 'zero', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictexnxgxmtujcmujanbn', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1icteymdb4nji3iwplcwpwzw', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1icti4ohgxnjijcmujanbn', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictk1mhg1mzqjcmujanbn', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictu2mhg3ntakzqlqcgc', 'zgkymde1lzewlza0l2zkl1n0yxj0dxayljq0mdvhlmpwzwpwcxrodw1ictywmhgzmzgjcmujanbn', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictexnxgxmtujcmujanbn', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1icteymdb4nji3iwplcwpwzw', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1icti4ohgxnjijcmujanbn', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictk1mhg1mzqjcmujanbn', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictu2mhg3ntakzqlqcgc', 'zgkymde1lzewlza0lzm1l2jpcmrfdgfudhj1lmu3zwmzlmpwzwpwcxrodw1ictywmhgzmzgjcmujanbn', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictexnxgxmtujcmujanbn', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1icteymdb4nji3iwplcwpwzw', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1icti4ohgxnjijcmujanbn', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictk1mhg1mzqjcmujanbn', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictu2mhg3ntakzqlqcgc', 'zgkymde1lzewlzaxlzhhl1rttfnjcmvlblnolmnkmgjklnbuzwpwcxrodw1ictywmhgzmzgjcmujanbn']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-150:-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** boolean, True by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 8759)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(lowercase=False)\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 37905)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with white', 'with win', 'with wine', 'with wiretap', 'with working', 'with younger', 'with your', 'witherspoon', 'witherspoon is', 'witherspoon will', 'within', 'within the', 'without', 'without bitterness', 'without buying', 'without clear', 'without explicit', 'without fight', 'without flexibility', 'without generating', 'without getting', 'without having', 'without its', 'without meeting', 'without needing', 'without positive', 'without proper', 'without single', 'without the', 'without touching', 'without trial', 'witness', 'witness says', 'wobble_d_whop', 'wobble_d_whop january', 'woes', 'woes on', 'woke', 'woke up', 'woman', 'woman campaign', 'woman for', 'woman if', 'woman image_src', 'woman in', 'woman is', 'woman og', 'woman the', 'woman to', 'woman twitter']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-1000:-950])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10.000000\n",
       "mean      0.420094\n",
       "std       0.117514\n",
       "min       0.250000\n",
       "25%       0.366477\n",
       "50%       0.409722\n",
       "75%       0.500000\n",
       "max       0.571429\n",
       "dtype: float64"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "pd.Series(cross_val_score(nb, X_dtm, y, cv=10)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_dtm = vect.fit_transform(X)\n",
    "    print('Features: ', X_dtm.shape[1])\n",
    "    nb = MultinomialNB()\n",
    "    print(pd.Series(cross_val_score(nb, X_dtm, y, cv=10)).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  37905\n",
      "count    10.000000\n",
      "mean      0.405808\n",
      "std       0.087028\n",
      "min       0.250000\n",
      "25%       0.375000\n",
      "50%       0.375000\n",
      "75%       0.440476\n",
      "max       0.571429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Stopword Removal\n",
    "\n",
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7710\n",
      "count    10.000000\n",
      "mean      0.355411\n",
      "std       0.085808\n",
      "min       0.250000\n",
      "25%       0.270833\n",
      "50%       0.369318\n",
      "75%       0.415179\n",
      "max       0.500000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'our', 'when', 'perhaps', 'same', 'whereby', 'will', 'co', 'un', 'has', 'under', 'take', 'among', 'an', 'a', 'becomes', 'fill', 'again', 'herself', 'nobody', 'sometimes', 'per', 'except', 'fify', 'her', 'hereafter', 'now', 'to', 'off', 'must', 'myself', 'such', 'noone', 'them', 'once', 'by', 'your', 'his', 'into', 'either', 'latter', 'most', 'whereas', 'mine', 'amoungst', 'there', 'rather', 'every', 'anywhere', 'never', 'for', 'or', 'few', 'afterwards', 'at', 'system', 'nothing', 'however', 'con', 'detail', 'call', 'well', 'another', 'hence', 'please', 'hers', 'twelve', 'being', 'least', 'what', 'whereupon', 'several', 'toward', 'done', 'further', 'only', 'whenever', 'beside', 'everything', 'ten', 'first', 'whence', 'beforehand', 'than', 'eight', 'which', 'sixty', 'together', 'themselves', 'own', 'that', 'during', 'yourself', 'almost', 'seem', 'throughout', 'wherein', 'describe', 'twenty', 'of', 'i', 'nine', 'everywhere', 'whoever', 'bill', 'hereby', 'do', 'seems', 'these', 'thin', 'cant', 'upon', 'in', 'since', 'ever', 'somewhere', 'fifteen', 'can', 'name', 'move', 'front', 'bottom', 'before', 'after', 'over', 'sincere', 'three', 'we', 'thus', 'though', 'yet', 'former', 'interest', 'why', 'one', 'above', 'latterly', 'neither', 'wherever', 'de', 'had', 'behind', 'via', 'whereafter', 'last', 'namely', 'am', 'not', 'yours', 'out', 'ourselves', 'due', 'two', 'would', 'cannot', 'thence', 'could', 'hereupon', 'keep', 'other', 'something', 'whom', 'whose', 'anyway', 'have', 'but', 'with', 'mill', 'they', 'be', 'no', 'very', 'if', 'empty', 'as', 'cry', 'him', 'from', 'always', 'meanwhile', 'across', 'and', 'its', 'show', 'up', 'both', 'find', 'much', 'any', 'itself', 'ltd', 'already', 'eg', 'those', 'top', 'ours', 'side', 'even', 'less', 'made', 'indeed', 'nowhere', 'whither', 'therein', 'then', 'until', 'forty', 'inc', 'been', 'it', 'next', 'around', 'are', 'otherwise', 'seeming', 'anyhow', 'put', 'part', 'should', 'some', 'who', 'nevertheless', 'became', 'on', 'within', 'without', 'hasnt', 'us', 'many', 'alone', 'none', 'may', 'thick', 'amongst', 'is', 'too', 'because', 'down', 'get', 'serious', 'whole', 'go', 'yourselves', 'herein', 'seemed', 'between', 'eleven', 'five', 'six', 'each', 'might', 'against', 'fire', 'becoming', 'anyone', 're', 'nor', 'others', 'formerly', 'whether', 'etc', 'therefore', 'thru', 'my', 'so', 'else', 'elsewhere', 'she', 'although', 'all', 'through', 'you', 'himself', 'enough', 'me', 'third', 'more', 'here', 'onto', 'besides', 'the', 'moreover', 'towards', 'full', 'were', 'thereafter', 'also', 'amount', 'below', 'everyone', 'how', 'often', 'see', 'whatever', 'somehow', 'where', 'while', 'thereby', 'along', 'give', 'become', 'about', 'was', 'anything', 'ie', 'someone', 'still', 'thereupon', 'this', 'sometime', 'beyond', 'back', 'found', 'hundred', 'four', 'mostly', 'couldnt', 'their', 'he'})\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Other CountVectorizer Options\n",
    "\n",
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100\n",
      "count    10.000000\n",
      "mean      0.375126\n",
      "std       0.168480\n",
      "min       0.125000\n",
      "25%       0.250000\n",
      "50%       0.401786\n",
      "75%       0.486111\n",
      "max       0.625000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01', '10', '11', '15', '1cd', '2015', '2016', '28', 'article', 'australian', 'author', 'best', 'big', 'business', 'campaign', 'com', 'company', 'conversion', 'cystic', 'daniel', 'day', 'description', 'digital', 'don', 'downey', 'entertainment', 'facebook', 'false', 'fibrosis', 'function', 'good', 'hot', 'http', 'https', 'image', 'initpage', 'instagram', 'internal', 'iron', 'jpg', 'jr', 'js', 'just', 'know', 'life', 'like', 'make', 'man', 'marketing', 'mashable', 'media', 'movie', 'movies', 'mshcdn', 'new', 'null', 'oct', 'og', 'old', 'open', 'paris', 'people', 'photo', 'pic', 'platform', 'police', 'posted', 'premiere', 'pu', 'rack', 'rdj', 'return', 'rights', 'rising', 'robert', 'said', 'sailthru', 'says', 'season', 'short_url', 'state', 'time', 'timer', 'title', 'topics', 'travel', 'true', 'trump', 'twitter', 'twttr', 'uncategorized', 'url', 've', 'watercooler', 'way', 'window', 'work', 'world', 'year', 'years']\n"
     ]
    }
   ],
   "source": [
    "# all 100 features\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  1000\n",
      "count    10.000000\n",
      "mean      0.405574\n",
      "std       0.130813\n",
      "min       0.250000\n",
      "25%       0.270833\n",
      "50%       0.414773\n",
      "75%       0.500000\n",
      "max       0.571429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7620\n",
      "count    10.000000\n",
      "mean      0.407594\n",
      "std       0.141763\n",
      "min       0.125000\n",
      "25%       0.366477\n",
      "50%       0.409722\n",
      "75%       0.500000\n",
      "max       0.571429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Introduction to TextBlob\n",
    "\n",
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): textblob in /home/al/anaconda3/envs/anaconda3/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied (use --upgrade to upgrade): nltk>=3.1 in /home/al/anaconda3/envs/anaconda3/lib/python3.5/site-packages (from textblob)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/al/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/al/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/al/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/al/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to /home/al/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to /home/al/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple's long and controversial ebook case has reached its final chapter — and it's not the happy ending the company wanted.\n",
      "The Supreme Court on Monday rejected an appeal filed by Apple to overturn a stinging ruling that it led a broad conspiracy with several major publishers to fix the price of e-books sold through its online bookstore.\n",
      "The court's decision means Apple now has no choice but to pay out $400 million to consumers and an additional $50 million in legal fees, according to the original settlement in 2014.\n",
      "SEE ALSO: Here's how Apple marshalled the entire tech industry in its fight with the FBI\n",
      "For Apple, the final verdict is more damaging to its reputation as a consumer-friendly brand, not to mention the legacy of its beloved founder Steve Jobs, than to its actual bottom line.\n",
      "To put the fine in context, the total $450 million payout is equal to about a little more than half the sales Apple generates on average each day, based on the $75.9 billion in revenue it reported in \n"
     ]
    }
   ],
   "source": [
    "# print the first text\n",
    "print(X[0][0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Apple', \"'s\", 'long', 'and', 'controversial', 'ebook', 'case', 'has', 'reached', 'its', 'final', 'chapter', '—', 'and', 'it', \"'s\", 'not', 'the', 'happy', 'ending', 'the', 'company', 'wanted', 'The', 'Supreme', 'Court', 'on', 'Monday', 'rejected', 'an', 'appeal', 'filed', 'by', 'Apple', 'to', 'overturn', 'a', 'stinging', 'ruling', 'that', 'it', 'led', 'a', 'broad', 'conspiracy', 'with', 'several', 'major', 'publishers', 'to'])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " Apple's long and controversial ebook case has reached its final chapter — and it's not the happy ending the company wanted.\"),\n",
       " Sentence(\"The Supreme Court on Monday rejected an appeal filed by Apple to overturn a stinging ruling that it led a broad conspiracy with several major publishers to fix the price of e-books sold through its online bookstore.\"),\n",
       " Sentence(\"The court's decision means Apple now has no choice but to pay out $400 million to consumers and an additional $50 million in legal fees, according to the original settlement in 2014.\"),\n",
       " Sentence(\"SEE ALSO: Here's how Apple marshalled the entire tech industry in its fight with the FBI\n",
       " For Apple, the final verdict is more damaging to its reputation as a consumer-friendly brand, not to mention the legacy of its beloved founder Steve Jobs, than to its actual bottom line.\"),\n",
       " Sentence(\"To put the fine in context, the total $450 million payout is equal to about a little more than half the sales Apple generates on average each day, based on the $75.9 billion in revenue it reported in the most recent quarter.\")]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"\n",
       "apple's long and controversial ebook case has reached its final chapter — and it's not the happy ending the company wanted.\n",
       "the supreme court on monday rejected an appeal filed by apple to overturn a stinging ruling that it led a broad conspiracy with several major publishers to fix the price of e-books sold through its online bookstore.\n",
       "the court's decision means apple now has no choice but to pay out $400 million to consumers and an additional $50 million in legal fees, according to the original settlement in 2014.\n",
       "see also: here's how apple marshalled the entire tech industry in its fight with the fbi\n",
       "for apple, the final verdict is more damaging to its reputation as a consumer-friendly brand, not to mention the legacy of its beloved founder steve jobs, than to its actual bottom line.\n",
       "to put the fine in context, the total $450 million payout is equal to about a little more than half the sales apple generates on average each day, based on the $75.9 billion in revenue it reported in \")"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Stemming and Lemmatization\n",
    "\n",
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['appl', \"'s\", 'long', 'and', 'controversi', 'ebook', 'case', 'has', 'reach', 'it', 'final', 'chapter', '—', 'and', 'it', \"'s\", 'not', 'the', 'happi', 'end', 'the', 'compani', 'want', 'the', 'suprem', 'court', 'on', 'monday', 'reject', 'an', 'appeal', 'file', 'by', 'appl', 'to', 'overturn', 'a', 'sting', 'rule', 'that', 'it', 'led', 'a', 'broad', 'conspiraci', 'with', 'sever', 'major', 'publish', 'to', 'fix', 'the', 'price', 'of', 'e-book', 'sold', 'through', 'it', 'onlin', 'bookstor', 'the', 'court', \"'s\", 'decis', 'mean', 'appl', 'now', 'has', 'no', 'choic', 'but', 'to', 'pay', 'out', '400', 'million', 'to', 'consum', 'and', 'an', 'addit', '50', 'million', 'in', 'legal', 'fee', 'accord', 'to', 'the', 'origin', 'settlement', 'in', '2014', 'see', 'also', 'here', \"'s\", 'how', 'appl', 'marshal', 'the', 'entir', 'tech', 'industri', 'in', 'it', 'fight', 'with', 'the', 'fbi', 'for', 'appl', 'the', 'final', 'verdict', 'is', 'more', 'damag', 'to', 'it', 'reput', 'as', 'a', 'consumer-friend', 'brand', 'not', 'to', 'mention', 'the', 'legaci', 'of', 'it', 'belov', 'founder', 'steve', 'job', 'than', 'to', 'it', 'actual', 'bottom', 'line', 'to', 'put', 'the', 'fine', 'in', 'context', 'the', 'total', '450', 'million', 'payout', 'is', 'equal', 'to', 'about', 'a', 'littl', 'more', 'than', 'half', 'the', 'sale', 'appl', 'generat', 'on', 'averag', 'each', 'day', 'base', 'on', 'the', '75.9', 'billion', 'in', 'revenu', 'it', 'report', 'in', 'the', 'most', 'recent', 'quarter', 'the', 'price-fix', 'episod', 'date', 'back', 'to', 'late', '2009', 'just', 'ahead', 'of', 'the', 'origin', 'ipad', 'launch', 'appl', 'and', 'it', 'founder', 'recogn', 'that', 'book', 'would', 'like', 'be', 'a', 'big', 'sell', 'point', 'of', 'the', 'tablet', 'began', 'court', 'what', 'were', 'then', 'the', 'big', 'five', 'book', 'publish', 'in', 'a', 'seri', 'of', 'e-mail', 'later', 'releas', 'by', 'the', 'u.', 'govern', 'job', 'person', 'persuad', 'publish', 'industri', 'execut', 'to', 're-think', 'the', 'flat', '9.99', 'e-book', 'price', 'previous', 'impos', 'by', 'amazon', 'then', 'and', 'now', 'the', 'giant', 'of', 'the', 'e-book', 'world', 'all', 'the', 'major', 'publish', 'tell', 'us', 'that', 'amazon', '9.99', 'price', 'for', 'new', 'releas', 'is', 'erod', 'the', 'valu', 'percept', 'of', 'their', 'product', 'in', 'custom', 'mind', 'and', 'they', 'do', 'not', 'want', 'this', 'practic', 'to', 'continu', 'for', 'new', 'releas', 'job', 'wrote', 'in', 'one', 'email', 'to', 'jame', 'murdoch', 'an', 'execut', 'at', 'news', 'corp', 'which', 'own', 'harper', 'collin', 'an', 'email', 'appl', 'ceo', 'steve', 'job', 'sent', 'to', 'news', 'corp', 'exec', 'jame', 'murdoch.imag', 'screengrab', 'mashableth', 'major', 'publish', 'unhappi', 'about', 'the', 'unfavor', 'term', 'they', \"'d\", 'agre', 'to', 'with', 'amazon', 'sign', 'on', 'to', 'appl', \"'s\", 'plan', 'and', 'later', 'use', 'the', 'new', 'competit', 'to', 'pressur', 'amazon', 'into', 'chang', 'it', 'own', 'price', 'structur', 'while', 'some', 'in', 'the', 'publish', 'industri', 'argu', 'this', 'move', 'help', 'break', 'up', 'amazon', \"'s\", 'potenti', 'monopoli', 'on', 'the', 'market', 'the', 'u.', 'govern', 'accus', 'appl', 'and', 'the', 'five', 'publish', 'of', 'collud', 'to', 'keep', 'price', 'high', 'the', 'publish', '—', 'hachett', 'harpercollin', 'macmillan', 'penguin', 'and', 'simon', 'schuster', '—', 'all', 'settl', 'with', 'the', 'depart', 'of', 'justic', 'befor', 'go', 'to', 'trial', 'onli', 'appl', 'arm', 'with', 'the', 'unwav', 'belief', 'in', 'it', 'own', 'right', 'argu', 'the', 'case', 'in', 'the', 'court', 'we', 'are', 'readi', 'to', 'distribut', 'the', 'court-mand', 'settlement', 'fund', 'to', 'kindl', 'custom', 'as', 'soon', 'as', \"we'r\", 'instruct', 'to', 'move', 'forward', 'a', 'spokesperson', 'for', 'amazon', 'said', 'in', 'a', 'statement', 'provid', 'to', 'mashabl', 'rep', 'for', 'appl', 'did', 'not', 'immedi', 'respond', 'to', 'our', 'request', 'for', 'comment', 'on', 'the', 'suprem', 'court', 'decis', 'howev', 'a', 'compani', 'statement', 'after', 'it', 'big', 'loss', 'in', 'court', 'in', '2013', 'say', 'it', 'all', 'when', 'we', 'introduc', 'the', 'ibookstor', 'in', '2010', 'we', 'gave', 'custom', 'more', 'choic', 'inject', 'much', 'need', 'innov', 'and', 'competit', 'into', 'the', 'market', 'break', 'amazon', \"'s\", 'monopolist', 'grip', 'on', 'the', 'publish', 'industri', 'appl', 'said', 'in', 'a', 'statement', 'at', 'the', 'time', 'we', 've', 'done', 'noth', 'wrong', 'the', 'u.', 'court', 'have', 'onc', 'again', 'determin', 'otherwis']\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', \"'s\", 'long', 'and', 'controversial', 'ebook', 'case', 'ha', 'reached', 'it', 'final', 'chapter', '—', 'and', 'it', \"'s\", 'not', 'the', 'happy', 'ending', 'the', 'company', 'wanted', 'The', 'Supreme', 'Court', 'on', 'Monday', 'rejected', 'an', 'appeal', 'filed', 'by', 'Apple', 'to', 'overturn', 'a', 'stinging', 'ruling', 'that', 'it', 'led', 'a', 'broad', 'conspiracy', 'with', 'several', 'major', 'publisher', 'to', 'fix', 'the', 'price', 'of', 'e-books', 'sold', 'through', 'it', 'online', 'bookstore', 'The', 'court', \"'s\", 'decision', 'mean', 'Apple', 'now', 'ha', 'no', 'choice', 'but', 'to', 'pay', 'out', '400', 'million', 'to', 'consumer', 'and', 'an', 'additional', '50', 'million', 'in', 'legal', 'fee', 'according', 'to', 'the', 'original', 'settlement', 'in', '2014', 'SEE', 'ALSO', 'Here', \"'s\", 'how', 'Apple', 'marshalled', 'the', 'entire', 'tech', 'industry', 'in', 'it', 'fight', 'with', 'the', 'FBI', 'For', 'Apple', 'the', 'final', 'verdict', 'is', 'more', 'damaging', 'to', 'it', 'reputation', 'a', 'a', 'consumer-friendly', 'brand', 'not', 'to', 'mention', 'the', 'legacy', 'of', 'it', 'beloved', 'founder', 'Steve', 'Jobs', 'than', 'to', 'it', 'actual', 'bottom', 'line', 'To', 'put', 'the', 'fine', 'in', 'context', 'the', 'total', '450', 'million', 'payout', 'is', 'equal', 'to', 'about', 'a', 'little', 'more', 'than', 'half', 'the', 'sale', 'Apple', 'generates', 'on', 'average', 'each', 'day', 'based', 'on', 'the', '75.9', 'billion', 'in', 'revenue', 'it', 'reported', 'in', 'the', 'most', 'recent', 'quarter', 'The', 'price-fixing', 'episode', 'date', 'back', 'to', 'late', '2009', 'just', 'ahead', 'of', 'the', 'original', 'iPad', 'launch', 'Apple', 'and', 'it', 'founder', 'recognizing', 'that', 'book', 'would', 'likely', 'be', 'a', 'big', 'selling', 'point', 'of', 'the', 'tablet', 'began', 'courting', 'what', 'were', 'then', 'the', 'big', 'five', 'book', 'publisher', 'In', 'a', 'series', 'of', 'e-mail', 'later', 'released', 'by', 'the', 'U.S', 'government', 'Jobs', 'personally', 'persuaded', 'publishing', 'industry', 'executive', 'to', 're-think', 'the', 'flat', '9.99', 'e-book', 'pricing', 'previously', 'imposed', 'by', 'Amazon', 'then', 'and', 'now', 'the', 'giant', 'of', 'the', 'e-book', 'world', 'All', 'the', 'major', 'publisher', 'tell', 'u', 'that', 'Amazon’s', '9.99', 'price', 'for', 'new', 'release', 'is', 'eroding', 'the', 'value', 'perception', 'of', 'their', 'product', 'in', 'customer’s', 'mind', 'and', 'they', 'do', 'not', 'want', 'this', 'practice', 'to', 'continue', 'for', 'new', 'release', 'Jobs', 'wrote', 'in', 'one', 'email', 'to', 'James', 'Murdoch', 'an', 'executive', 'at', 'News', 'Corp', 'which', 'owns', 'Harper', 'Collins', 'An', 'email', 'Apple', 'CEO', 'Steve', 'Jobs', 'sent', 'to', 'News', 'Corp', 'exec', 'James', 'Murdoch.Image', 'screengrab', 'mashableThe', 'major', 'publisher', 'unhappy', 'about', 'the', 'unfavorable', 'term', 'they', \"'d\", 'agreed', 'to', 'with', 'Amazon', 'signed', 'on', 'to', 'Apple', \"'s\", 'plan', 'and', 'later', 'used', 'the', 'new', 'competition', 'to', 'pressure', 'Amazon', 'into', 'changing', 'it', 'own', 'pricing', 'structure', 'While', 'some', 'in', 'the', 'publishing', 'industry', 'argued', 'this', 'move', 'helped', 'break', 'up', 'Amazon', \"'s\", 'potential', 'monopoly', 'on', 'the', 'market', 'the', 'U.S', 'government', 'accused', 'Apple', 'and', 'the', 'five', 'publisher', 'of', 'colluding', 'to', 'keep', 'price', 'high', 'The', 'publisher', '—', 'Hachette', 'HarperCollins', 'Macmillan', 'Penguin', 'and', 'Simon', 'Schuster', '—', 'all', 'settled', 'with', 'the', 'Department', 'of', 'Justice', 'before', 'going', 'to', 'trial', 'Only', 'Apple', 'armed', 'with', 'the', 'unwavering', 'belief', 'in', 'it', 'own', 'rightness', 'argued', 'the', 'case', 'in', 'the', 'court', 'We', 'are', 'ready', 'to', 'distribute', 'the', 'court-mandated', 'settlement', 'fund', 'to', 'Kindle', 'customer', 'a', 'soon', 'a', 'we’re', 'instructed', 'to', 'move', 'forward', 'a', 'spokesperson', 'for', 'Amazon', 'said', 'in', 'a', 'statement', 'provided', 'to', 'Mashable', 'Reps', 'for', 'Apple', 'did', 'not', 'immediately', 'respond', 'to', 'our', 'request', 'for', 'comment', 'on', 'the', 'Supreme', 'Court', 'decision', 'However', 'a', 'company', 'statement', 'after', 'it', 'big', 'loss', 'in', 'court', 'in', '2013', 'say', 'it', 'all', 'When', 'we', 'introduced', 'the', 'iBookstore', 'in', '2010', 'we', 'gave', 'customer', 'more', 'choice', 'injecting', 'much', 'needed', 'innovation', 'and', 'competition', 'into', 'the', 'market', 'breaking', 'Amazon', \"'s\", 'monopolistic', 'grip', 'on', 'the', 'publishing', 'industry', 'Apple', 'said', 'in', 'a', 'statement', 'at', 'the', 'time', 'We', \"'ve\", 'done', 'nothing', 'wrong', 'The', 'U.S', 'court', 'have', 'once', 'again', 'determined', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', \"'s\", 'long', 'and', 'controversial', 'ebook', 'case', 'have', 'reach', 'its', 'final', 'chapter', '—', 'and', 'it', \"'s\", 'not', 'the', 'happy', 'end', 'the', 'company', 'want', 'The', 'Supreme', 'Court', 'on', 'Monday', 'reject', 'an', 'appeal', 'file', 'by', 'Apple', 'to', 'overturn', 'a', 'sting', 'rule', 'that', 'it', 'lead', 'a', 'broad', 'conspiracy', 'with', 'several', 'major', 'publishers', 'to', 'fix', 'the', 'price', 'of', 'e-books', 'sell', 'through', 'its', 'online', 'bookstore', 'The', 'court', \"'s\", 'decision', 'mean', 'Apple', 'now', 'have', 'no', 'choice', 'but', 'to', 'pay', 'out', '400', 'million', 'to', 'consumers', 'and', 'an', 'additional', '50', 'million', 'in', 'legal', 'fee', 'accord', 'to', 'the', 'original', 'settlement', 'in', '2014', 'SEE', 'ALSO', 'Here', \"'s\", 'how', 'Apple', 'marshal', 'the', 'entire', 'tech', 'industry', 'in', 'its', 'fight', 'with', 'the', 'FBI', 'For', 'Apple', 'the', 'final', 'verdict', 'be', 'more', 'damage', 'to', 'its', 'reputation', 'as', 'a', 'consumer-friendly', 'brand', 'not', 'to', 'mention', 'the', 'legacy', 'of', 'its', 'beloved', 'founder', 'Steve', 'Jobs', 'than', 'to', 'its', 'actual', 'bottom', 'line', 'To', 'put', 'the', 'fine', 'in', 'context', 'the', 'total', '450', 'million', 'payout', 'be', 'equal', 'to', 'about', 'a', 'little', 'more', 'than', 'half', 'the', 'sales', 'Apple', 'generate', 'on', 'average', 'each', 'day', 'base', 'on', 'the', '75.9', 'billion', 'in', 'revenue', 'it', 'report', 'in', 'the', 'most', 'recent', 'quarter', 'The', 'price-fixing', 'episode', 'date', 'back', 'to', 'late', '2009', 'just', 'ahead', 'of', 'the', 'original', 'iPad', 'launch', 'Apple', 'and', 'its', 'founder', 'recognize', 'that', 'book', 'would', 'likely', 'be', 'a', 'big', 'sell', 'point', 'of', 'the', 'tablet', 'begin', 'court', 'what', 'be', 'then', 'the', 'big', 'five', 'book', 'publishers', 'In', 'a', 'series', 'of', 'e-mail', 'later', 'release', 'by', 'the', 'U.S', 'government', 'Jobs', 'personally', 'persuade', 'publish', 'industry', 'executives', 'to', 're-think', 'the', 'flat', '9.99', 'e-book', 'price', 'previously', 'impose', 'by', 'Amazon', 'then', 'and', 'now', 'the', 'giant', 'of', 'the', 'e-book', 'world', 'All', 'the', 'major', 'publishers', 'tell', 'us', 'that', 'Amazon’s', '9.99', 'price', 'for', 'new', 'release', 'be', 'erode', 'the', 'value', 'perception', 'of', 'their', 'products', 'in', 'customer’s', 'mind', 'and', 'they', 'do', 'not', 'want', 'this', 'practice', 'to', 'continue', 'for', 'new', 'release', 'Jobs', 'write', 'in', 'one', 'email', 'to', 'James', 'Murdoch', 'an', 'executive', 'at', 'News', 'Corp', 'which', 'own', 'Harper', 'Collins', 'An', 'email', 'Apple', 'CEO', 'Steve', 'Jobs', 'send', 'to', 'News', 'Corp', 'exec', 'James', 'Murdoch.Image', 'screengrab', 'mashableThe', 'major', 'publishers', 'unhappy', 'about', 'the', 'unfavorable', 'term', 'they', \"'d\", 'agree', 'to', 'with', 'Amazon', 'sign', 'on', 'to', 'Apple', \"'s\", 'plan', 'and', 'later', 'use', 'the', 'new', 'competition', 'to', 'pressure', 'Amazon', 'into', 'change', 'its', 'own', 'price', 'structure', 'While', 'some', 'in', 'the', 'publish', 'industry', 'argue', 'this', 'move', 'help', 'break', 'up', 'Amazon', \"'s\", 'potential', 'monopoly', 'on', 'the', 'market', 'the', 'U.S', 'government', 'accuse', 'Apple', 'and', 'the', 'five', 'publishers', 'of', 'collude', 'to', 'keep', 'price', 'high', 'The', 'publishers', '—', 'Hachette', 'HarperCollins', 'Macmillan', 'Penguin', 'and', 'Simon', 'Schuster', '—', 'all', 'settle', 'with', 'the', 'Department', 'of', 'Justice', 'before', 'go', 'to', 'trial', 'Only', 'Apple', 'arm', 'with', 'the', 'unwavering', 'belief', 'in', 'its', 'own', 'rightness', 'argue', 'the', 'case', 'in', 'the', 'court', 'We', 'be', 'ready', 'to', 'distribute', 'the', 'court-mandated', 'settlement', 'fund', 'to', 'Kindle', 'customers', 'as', 'soon', 'as', 'we’re', 'instruct', 'to', 'move', 'forward', 'a', 'spokesperson', 'for', 'Amazon', 'say', 'in', 'a', 'statement', 'provide', 'to', 'Mashable', 'Reps', 'for', 'Apple', 'do', 'not', 'immediately', 'respond', 'to', 'our', 'request', 'for', 'comment', 'on', 'the', 'Supreme', 'Court', 'decision', 'However', 'a', 'company', 'statement', 'after', 'its', 'big', 'loss', 'in', 'court', 'in', '2013', 'say', 'it', 'all', 'When', 'we', 'introduce', 'the', 'iBookstore', 'in', '2010', 'we', 'give', 'customers', 'more', 'choice', 'inject', 'much', 'need', 'innovation', 'and', 'competition', 'into', 'the', 'market', 'break', 'Amazon', \"'s\", 'monopolistic', 'grip', 'on', 'the', 'publish', 'industry', 'Apple', 'say', 'in', 'a', 'statement', 'at', 'the', 'time', 'We', \"'ve\", 'do', 'nothing', 'wrong', 'The', 'U.S', 'court', 'have', 'once', 'again', 'determine', 'otherwise']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  7628\n",
      "count    10.000000\n",
      "mean      0.405808\n",
      "std       0.105100\n",
      "min       0.250000\n",
      "25%       0.366477\n",
      "50%       0.401786\n",
      "75%       0.486111\n",
      "max       0.571429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df_ = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df_.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0    0  0.333333  0.0       0        1    1\n",
       "1    1  0.333333  0.5       0        0    0\n",
       "2    0  0.333333  0.5       2        0    0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Using TF-IDF to Summarize a text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 7710)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "dtm = vect.fit_transform(X)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose a random text\n",
    "review_id = 40\n",
    "review_text = X[review_id]\n",
    "review_length = len(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary of words and their TF-IDF scores\n",
    "word_scores = {}\n",
    "for word in TextBlob(review_text).words:\n",
    "    word = word.lower()\n",
    "    if word in features:\n",
    "        word_scores[word] = dtm[review_id, features.index(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "sanders\n",
      "iowa\n",
      "precinct\n",
      "coin\n",
      "moines\n"
     ]
    }
   ],
   "source": [
    "# print words with the top 5 TF-IDF scores\n",
    "print('TOP SCORING WORDS:')\n",
    "top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for word, score in top_scores:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANDOM WORDS:\n",
      "dollars\n",
      "pitching\n",
      "eventually\n",
      "senator\n",
      "edged\n"
     ]
    }
   ],
   "source": [
    "# print 5 random words\n",
    "print('\\n' + 'RANDOM WORDS:')\n",
    "random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "for word in random_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Apple's long and controversial ebook case has reached its final chapter — and it's not the happy ending the company wanted.\n",
      "The Supreme Court on Monday rejected an appeal filed by Apple to overturn a stinging ruling that it led a broad conspiracy with several major publishers to fix the price of e-books sold through its online bookstore.\n",
      "The court's decision means Apple now has no choice but to pay out $400 million to consumers and an additional $50 million in legal fees, according to the original settlement in 2014.\n",
      "SEE ALSO: Here's how Apple marshalled the entire tech industry in its fight with the FBI\n",
      "For Apple, the final verdict is more damaging to its reputation as a consumer-friendly brand, not to mention the legacy of its beloved founder Steve Jobs, than to its actual bottom line.\n",
      "To put the fine in context, the total $450 million payout is equal to about a little more than half the sales Apple generates on average each day, based on the $75.9 billion in revenue it reported in \n"
     ]
    }
   ],
   "source": [
    "print(review[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11156714200831849"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "df['sentiment'] = df['text'].apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbf2a9923c8>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEaCAYAAAAR0SDgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf2a974f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "df.boxplot(column='sentiment', by='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     Today's digital marketing experts must have a ...\n",
       "6     LOS ANGELES — Natalie Cole, the accomplished R...\n",
       "11    As the battle between Apple and the FBI heats ...\n",
       "12    If you're still painstakingly killing all your...\n",
       "13    \\nApple's annual World Gallery showcases some ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text with most positive sentiment\n",
    "df[df.sentiment >= df.sentiment.quantile(q=0.75)].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     Analysis\\n\\n\\n\\n\\n\\nThere is a reason that Don...\n",
       "15    Since the Nov. 13 terrorist attacks in Paris, ...\n",
       "17    Since the Nov. 13 terrorist attacks in Paris, ...\n",
       "18    SEOUL, South Korea — Samsung appointed a new m...\n",
       "23    Updated December 14, 4:50 AM ET with tweet on ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "df[df.sentiment <= df.sentiment.quantile(q=0.25)].text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- NLP is a gigantic field\n",
    "- Understanding the basics broadens the types of data you can work with\n",
    "- Simple techniques go a long way\n",
    "- Use scikit-learn for NLP whenever possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "_merged"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
