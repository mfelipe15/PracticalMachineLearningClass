{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/09/16/sprints-upgrade...</td>\n",
       "      <td>479</td>\n",
       "      <td>11</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.407946</td>\n",
       "      <td>1</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305139</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2014/12/06/eric-garner-pro...</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/06/06/human-scale-inv...</td>\n",
       "      <td>581</td>\n",
       "      <td>6</td>\n",
       "      <td>604</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.675516</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.215873</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2014/11/24/beyonce-spotify...</td>\n",
       "      <td>44</td>\n",
       "      <td>16</td>\n",
       "      <td>708</td>\n",
       "      <td>0.474286</td>\n",
       "      <td>1</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.376852</td>\n",
       "      <td>-0.875000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/03/03/superheroic-let...</td>\n",
       "      <td>676</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/09/16/sprints-upgrade...        479   \n",
       "1  http://mashable.com/2014/12/06/eric-garner-pro...         31   \n",
       "2  http://mashable.com/2013/06/06/human-scale-inv...        581   \n",
       "3  http://mashable.com/2014/11/24/beyonce-spotify...         44   \n",
       "4  http://mashable.com/2013/03/03/superheroic-let...        676   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0              11              1045         0.407946                 1   \n",
       "1              14                 0         0.000000                 0   \n",
       "2               6               604         0.470000                 1   \n",
       "3              16               708         0.474286                 1   \n",
       "4               7                30         1.000000                 1   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs   ...     \\\n",
       "0                  0.617021         29               3         1   ...      \n",
       "1                  0.000000          0               0         0   ...      \n",
       "2                  0.675516          9               0         1   ...      \n",
       "3                  0.675325          5               2         2   ...      \n",
       "4                  1.000000          2               1         0   ...      \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.033333                    1.0              -0.305139   \n",
       "1               0.000000                    0.0               0.000000   \n",
       "2               0.100000                    1.0              -0.215873   \n",
       "3               0.062500                    0.5              -0.376852   \n",
       "4               0.250000                    0.7               0.000000   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0              -0.700000              -0.125000                 0.0   \n",
       "1               0.000000               0.000000                 1.0   \n",
       "2              -0.666667              -0.071429                 0.1   \n",
       "3              -0.875000              -0.100000                 0.4   \n",
       "4               0.000000               0.000000                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.0                     0.5   \n",
       "1                      -0.5                     0.5   \n",
       "2                       0.0                     0.4   \n",
       "3                      -0.2                     0.1   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  Popular  \n",
       "0                           0.0        1  \n",
       "1                           0.5        0  \n",
       "2                           0.0        0  \n",
       "3                           0.2        0  \n",
       "4                           0.0        1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('14_mashable_train_df.csv', index_col=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23786, 61)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(['url', 'Popular'], axis=1)\n",
    "y = train_df['Popular'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 16.1 (2 points)\n",
    "\n",
    "Estimate a Logistic Regression, GaussianNB, K-nearest neighbors and a Decision Tree **Classifiers**\n",
    "\n",
    "Evaluate using the following metrics:\n",
    "* Accuracy\n",
    "* F1-Score\n",
    "* F_Beta-Score (Beta=2)\n",
    "\n",
    "Comment about the results\n",
    "\n",
    "Combine the classifiers and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = {'lr': LogisticRegression(),\n",
    "          'dt': DecisionTreeClassifier(),\n",
    "          'nb': GaussianNB(),\n",
    "          'nn': KNeighborsClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)\n",
    "\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_train, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict test for each model\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.358189870299\n",
      "nn 0.371554571673\n",
      "nb 0.393959775474\n",
      "dt 0.461936034478\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for model in models.keys():\n",
    "    print(model,np.sqrt(mean_squared_error(y_pred[model], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>nn</th>\n",
       "      <th>nb</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lr  nn  nb  dt\n",
       "4754    0   0   0   0\n",
       "10360   0   0   0   0\n",
       "13048   0   0   0   0\n",
       "1912    0   0   1   0\n",
       "3323    0   0   0   0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr precision 0.00264900662252\n",
      "lr recall 0.166666666667\n",
      "lr f1.score 0.00521512385919\n",
      "lr fbeta 0.0359028511088\n",
      "nn precision 0.0331125827815\n",
      "nn recall 0.215517241379\n",
      "nn f1.score 0.0574052812859\n",
      "nn fbeta 0.162772883953\n",
      "nb precision 0.104635761589\n",
      "nb recall 0.242331288344\n",
      "nb f1.score 0.146160962072\n",
      "nb fbeta 0.224920448836\n",
      "dt precision 0.225165562914\n",
      "dt recall 0.19906323185\n",
      "dt f1.score 0.211311373524\n",
      "dt fbeta 0.20042998821\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for model in models.keys():\n",
    "    print(model, 'precision',metrics.precision_score(y_pred[model],y_test))\n",
    "    print(model, 'recall', metrics.recall_score(y_pred[model],y_test))\n",
    "    print(model, 'f1.score',metrics.f1_score(y_pred[model],y_test))\n",
    "    print(model, 'fbeta',metrics.fbeta_score(y_pred[model],y_test,beta=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de los arboles de decision es significativamente mejor que los demás metodos. El mejor recall que es la tasa de verdaderos positivos es de naive bayes.\n",
    "Los arboles de decisión también tienen el mejor f1.score y el mejor fbeta es de naive bayes.\n",
    "Con esto podemos ver que a regresión logistica no tiene un buen ajuste en este caso al igual que KNN.\n",
    "Naive Bayes genera buenos resultados pero definitivamente los arboles de decisión son los que mejor se ajustan a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 16.2\n",
    "\n",
    "Apply random-undersampling with a target percentage of 0.5\n",
    "\n",
    "how does the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UnderSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "\n",
    "    n_samples_0_new =  n_samples_1 / target_percentage - n_samples_1\n",
    "    n_samples_0_new_per = n_samples_0_new / n_samples_0\n",
    "\n",
    "    filter_ = y == 0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rand_1 = np.random.binomial(n=1, p=n_samples_0_new_per, size=n_samples)\n",
    "    \n",
    "    filter_ = filter_ & rand_1\n",
    "    filter_ = filter_ | (y == 1)\n",
    "    filter_ = filter_.astype(bool)\n",
    "    \n",
    "    return X[filter_], y[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr fbeta 0.244509358433\n",
      "nn fbeta 0.193039443155\n",
      "nb fbeta 0.216116085211\n",
      "dt fbeta 0.180312311276\n"
     ]
    }
   ],
   "source": [
    "X_u, y_u = UnderSampling(X_train, y_train,0.5,666)\n",
    "\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_u, y_u)\n",
    "    \n",
    "# predict test for each model\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)\n",
    "    \n",
    "\n",
    "from sklearn import metrics\n",
    "for model in models.keys():\n",
    "    print(model, 'fbeta',metrics.fbeta_score(y_pred[model],y_test,beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este metodo la regresión logistica obtiene resultados muy buenos comparados con los que tenia ya que pasa de tener un fbeta de 0.035 a uno de 0.244 lo que es un cambio significativo, además se vuelve el de mejor puntaje (en fbeta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 16.3 (3 points)\n",
    "\n",
    "For each model estimate a BaggingClassifier of 100 models using the under sampled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "treeclas = DecisionTreeClassifier(max_depth=None, random_state=123)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagreg = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, \n",
    "                          bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and predict\n",
    "bagreg.fit(X_u, y_u)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fbeta 0.251202041017\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('fbeta',metrics.fbeta_score(y_pred,y_test,beta=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 16.4 (2 points)\n",
    "\n",
    "Using the under-sampled dataset\n",
    "\n",
    "Evaluate a RandomForestClassifier and compare the results\n",
    "\n",
    "change n_estimators=100, what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfreg = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfreg.fit(X_u, y_u)\n",
    "y_pred = rfreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fbeta 0.246974318607\n"
     ]
    }
   ],
   "source": [
    "print('fbeta',metrics.fbeta_score(y_pred,y_test,beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_non_stop_words</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data_channel_is_lifestyle</td>\n",
       "      <td>0.001470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weekday_is_saturday</td>\n",
       "      <td>0.002036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>weekday_is_sunday</td>\n",
       "      <td>0.002550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data_channel_is_socmed</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data_channel_is_tech</td>\n",
       "      <td>0.002837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data_channel_is_bus</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data_channel_is_entertainment</td>\n",
       "      <td>0.002940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>weekday_is_thursday</td>\n",
       "      <td>0.003072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>0.003098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weekday_is_tuesday</td>\n",
       "      <td>0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>weekday_is_wednesday</td>\n",
       "      <td>0.003223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>weekday_is_monday</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>weekday_is_friday</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data_channel_is_world</td>\n",
       "      <td>0.004056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>kw_min_min</td>\n",
       "      <td>0.004062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>kw_max_max</td>\n",
       "      <td>0.005356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>max_positive_polarity</td>\n",
       "      <td>0.009511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>num_keywords</td>\n",
       "      <td>0.010564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min_positive_polarity</td>\n",
       "      <td>0.010746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>num_videos</td>\n",
       "      <td>0.011354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>abs_title_subjectivity</td>\n",
       "      <td>0.011496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>abs_title_sentiment_polarity</td>\n",
       "      <td>0.012384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num_self_hrefs</td>\n",
       "      <td>0.012962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>title_sentiment_polarity</td>\n",
       "      <td>0.013313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>max_negative_polarity</td>\n",
       "      <td>0.013811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>min_negative_polarity</td>\n",
       "      <td>0.014020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>num_imgs</td>\n",
       "      <td>0.014046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>title_subjectivity</td>\n",
       "      <td>0.014301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n_tokens_title</td>\n",
       "      <td>0.014458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>kw_min_max</td>\n",
       "      <td>0.018290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>rate_negative_words</td>\n",
       "      <td>0.019288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>rate_positive_words</td>\n",
       "      <td>0.019574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num_hrefs</td>\n",
       "      <td>0.021177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>global_rate_negative_words</td>\n",
       "      <td>0.022516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>kw_min_avg</td>\n",
       "      <td>0.023312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>avg_negative_polarity</td>\n",
       "      <td>0.023448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>global_rate_positive_words</td>\n",
       "      <td>0.023685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>kw_max_min</td>\n",
       "      <td>0.023758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>global_sentiment_polarity</td>\n",
       "      <td>0.023995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>self_reference_max_shares</td>\n",
       "      <td>0.024839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LDA_04</td>\n",
       "      <td>0.025126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LDA_00</td>\n",
       "      <td>0.025260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_unique_tokens</td>\n",
       "      <td>0.025877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>avg_positive_polarity</td>\n",
       "      <td>0.026001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_non_stop_unique_tokens</td>\n",
       "      <td>0.026143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LDA_01</td>\n",
       "      <td>0.026321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>kw_avg_max</td>\n",
       "      <td>0.026636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>global_subjectivity</td>\n",
       "      <td>0.026665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_tokens_content</td>\n",
       "      <td>0.026676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>kw_avg_min</td>\n",
       "      <td>0.026949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>self_reference_min_shares</td>\n",
       "      <td>0.028071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>timedelta</td>\n",
       "      <td>0.028566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>self_reference_avg_sharess</td>\n",
       "      <td>0.028702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>average_token_length</td>\n",
       "      <td>0.029018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LDA_03</td>\n",
       "      <td>0.031551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LDA_02</td>\n",
       "      <td>0.031689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>kw_max_avg</td>\n",
       "      <td>0.050366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kw_avg_avg</td>\n",
       "      <td>0.053033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          feature  importance\n",
       "4                n_non_stop_words    0.000173\n",
       "12      data_channel_is_lifestyle    0.001470\n",
       "35            weekday_is_saturday    0.002036\n",
       "36              weekday_is_sunday    0.002550\n",
       "15         data_channel_is_socmed    0.002713\n",
       "16           data_channel_is_tech    0.002837\n",
       "14            data_channel_is_bus    0.002911\n",
       "13  data_channel_is_entertainment    0.002940\n",
       "33            weekday_is_thursday    0.003072\n",
       "37                     is_weekend    0.003098\n",
       "31             weekday_is_tuesday    0.003207\n",
       "32           weekday_is_wednesday    0.003223\n",
       "30              weekday_is_monday    0.003396\n",
       "34              weekday_is_friday    0.003400\n",
       "17          data_channel_is_world    0.004056\n",
       "18                     kw_min_min    0.004062\n",
       "22                     kw_max_max    0.005356\n",
       "51          max_positive_polarity    0.009511\n",
       "11                   num_keywords    0.010564\n",
       "50          min_positive_polarity    0.010746\n",
       "9                      num_videos    0.011354\n",
       "57         abs_title_subjectivity    0.011496\n",
       "58   abs_title_sentiment_polarity    0.012384\n",
       "7                  num_self_hrefs    0.012962\n",
       "56       title_sentiment_polarity    0.013313\n",
       "54          max_negative_polarity    0.013811\n",
       "53          min_negative_polarity    0.014020\n",
       "8                        num_imgs    0.014046\n",
       "55             title_subjectivity    0.014301\n",
       "1                  n_tokens_title    0.014458\n",
       "21                     kw_min_max    0.018290\n",
       "48            rate_negative_words    0.019288\n",
       "47            rate_positive_words    0.019574\n",
       "6                       num_hrefs    0.021177\n",
       "46     global_rate_negative_words    0.022516\n",
       "24                     kw_min_avg    0.023312\n",
       "52          avg_negative_polarity    0.023448\n",
       "45     global_rate_positive_words    0.023685\n",
       "19                     kw_max_min    0.023758\n",
       "44      global_sentiment_polarity    0.023995\n",
       "28      self_reference_max_shares    0.024839\n",
       "42                         LDA_04    0.025126\n",
       "38                         LDA_00    0.025260\n",
       "3                 n_unique_tokens    0.025877\n",
       "49          avg_positive_polarity    0.026001\n",
       "5        n_non_stop_unique_tokens    0.026143\n",
       "39                         LDA_01    0.026321\n",
       "23                     kw_avg_max    0.026636\n",
       "43            global_subjectivity    0.026665\n",
       "2                n_tokens_content    0.026676\n",
       "20                     kw_avg_min    0.026949\n",
       "27      self_reference_min_shares    0.028071\n",
       "0                       timedelta    0.028566\n",
       "29     self_reference_avg_sharess    0.028702\n",
       "10           average_token_length    0.029018\n",
       "41                         LDA_03    0.031551\n",
       "40                         LDA_02    0.031689\n",
       "25                     kw_max_avg    0.050366\n",
       "26                     kw_avg_avg    0.053033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'feature':X.columns, 'importance':rfreg.feature_importances_}).sort_values('importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "x_u_=rfreg.transform(X_u, threshold='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4683, 29)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_u_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/utils/__init__.py:93: DeprecationWarning: Function transform is deprecated; Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_test_ = rfreg.transform(X_test, threshold='mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
