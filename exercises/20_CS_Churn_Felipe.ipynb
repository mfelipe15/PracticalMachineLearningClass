{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 20 - CostSensitive Churn\n",
    "\n",
    "[paper](http://download.springer.com/static/pdf/125/art%253A10.1186%252Fs40165-015-0014-6.pdf?originUrl=http%3A%2F%2Fdecisionanalyticsjournal.springeropen.com%2Farticle%2F10.1186%2Fs40165-015-0014-6&token2=exp=1462974790~acl=%2Fstatic%2Fpdf%2F125%2Fart%25253A10.1186%25252Fs40165-015-0014-6.pdf*~hmac=05041d990b7e5a5e70d6efc1fbb29c2a380465c6edc84be1feda1b6d49588a1a)\n",
    "[slides](http://www.slideshare.net/albahnsen/maximizing-a-churn-campaigns-profitability-with-cost-sensitive-predictive-analytics)\n",
    "\n",
    "Customer churn predictive modeling deals with predicting the probability of a customer defecting \n",
    "using historical, behavioral and socio-economical information. This tool is of great benefit to \n",
    "subscription based companies allowing them to maximize the results of retention campaigns. The \n",
    "problem of churn predictive modeling has been widely studied by the data mining and machine learning\n",
    "communities. It is usually tackled by using classification algorithms in order to learn the \n",
    "different patterns of both the churners and non-churners. Nevertheless, current state-of-the-art \n",
    "classification algorithms are not well aligned with commercial goals, in the sense that, the models \n",
    "miss to include the real financial costs and benefits during the training and evaluation phases. In \n",
    "the case of churn, evaluating a model based on a traditional measure such as accuracy or predictive \n",
    "power, does not yield to the best results when measured by the actual financial cost, i.e., \n",
    "investment per subscriber on a loyalty campaign and the financial impact of failing to detect a \n",
    "real churner versus wrongly predicting a non-churner as a churner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main objectives of subscription-based companies are to  acquire new subscribers and \n",
    "retain those they already have, mainly because profits are directly linked with the number of \n",
    "subscribers.  In order to maximize the profit, companies must increase the customer base by \n",
    "incrementing sales  while decreasing the number of churners. Furthermore, it is common knowledge \n",
    "that retaining a  customer is about five times less expensive than acquiring a new one , this creates  pressure to have better and more effective churn campaigns.\n",
    "\n",
    "A typical churn campaign consists in identifying from the current customer base which ones are \n",
    "more likely to leave the company, and make an offer in order to avoid that behavior.\n",
    "With this in mind the companies use intelligence to create and improve retention and collection\n",
    "strategies. In the first case, this usually implies an offer that can be either a discount or a \n",
    "free upgrade during certain span of time. In both cases the company has to \tassume a cost for that \n",
    "offer, therefore, accurate prediction of the churners becomes important. The logic of this flow is \n",
    "shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig1](ch5_fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The churn campaign process starts with the sales that every month increase the customer \n",
    "base, however, monthly there is a group of customers that decide to leave the company for many \n",
    "reasons. Then the objective of a churn model is to identify those customers before they take the \n",
    "decision of defecting.\n",
    "\n",
    "Using a churn model, those customers more likely to leave are predicted as churners and \n",
    "an offer is made in order to retain them. However, it is known that not all customers will accept \n",
    "the offer, in the case when a customer is planning to defect, it is possible that the offer is not \n",
    "good enough to retain him or that the reason for defecting can not be influenced by an offer.\n",
    "Using historical information, it is estimated that a customer will accept the offer with \n",
    "probability $\\gamma$.\n",
    "On the other hand, there is the case in which the churn model misclassified a non-churner as \n",
    "churner, also known as false positives, in that case the customer will always accept the offer that \n",
    "means and additional cost to the company since those misclassified customers do not have the \n",
    "intentions of leaving.\n",
    "\n",
    "In the case were the churn model predicts customers as non-churners, there is also the possibility \n",
    "of a misclassification, in this case an actual churner is predicted as non-churner, since \n",
    "these customers do not receive an offer and they will leave the company, these cases are known as \n",
    "false negatives. Lastly, there is the case were the customers are actually non-churners, then \n",
    "there is no need to make a retention offer to these customers since they will continue to be part \n",
    "of the customer base.\n",
    "\n",
    "It can be seen that a churn campaign (or churn model) has three main points. First, avoid false \n",
    "positives since there is a financial cost of making an offer where it is not needed. Second, find \n",
    "the right offer to give to those customers identified as churners. And lastly, to decrease \n",
    "the number of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure, the financial impact of a churn model is shown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig1](ch5_fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note than we take \n",
    "into account the costs and not the profit in each case.\n",
    "When a customer is predicted to be a churner, an offer is made with the objective of avoiding \n",
    "the customer defecting. However, if a customer is actually a churner, he may or not accept the \n",
    "offer with a probability $\\gamma_i$. If the customer accepts the offer, the financial impact is \n",
    "equal to the cost of the offer ($C_{o_i}$) plus the administrative cost of contacting the \n",
    "customer ($C_a$). On the other hand, if the customer declines the offer, the cost is the \n",
    "expected \tincome that the clients would otherwise generate, also called customer lifetime value \n",
    "($CLV_i$), \tplus $C_a$. Lastly, if the customer is not actually a churner, he will be happy to \n",
    "accept the \toffer and the cost will be $C_{o_i}$ plus $C_a$.\n",
    "\t\n",
    "In the case that the customer is predicted as non-churner, there are two possible outcomes. \n",
    "Either the customer is not a churner, then the cost is zero, or the customer is a churner and the \n",
    "cost is $CLV_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|  \t| Actual Positive ($y_i=1$)  \t|  Actual Negative \t($y_i=0$)|\n",
    "|---\t|:-:\t|:-:\t|\n",
    "|   Predicted Positive ($c_i=1$)\t|   $C_{TP_i}=\\gamma_iC_{o_i}+(1-\\gamma_i)(CLV_i+C_a)$\t| $C_{FP_i}=C_{o_i}+C_a$ |\n",
    "|  Predicted Negative  ($c_i=0$) \t|   $C_{FN_i}=CLV_i$\t| $C_{TN_i}=0$\t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('cost_sensitive_classification_churn.csv.zip', 'r') as z:\n",
    "    f = z.open('cost_sensitive_classification_churn.csv')\n",
    "    data = pd.io.parsers.read_table(f, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>C_FP</th>\n",
       "      <th>C_FN</th>\n",
       "      <th>C_TP</th>\n",
       "      <th>C_TN</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1028.571429</td>\n",
       "      <td>121.828571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>53.428571</td>\n",
       "      <td>1028.571429</td>\n",
       "      <td>82.742857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>66.285714</td>\n",
       "      <td>1285.714286</td>\n",
       "      <td>102.928571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1285.714286</td>\n",
       "      <td>151.785714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>53.428571</td>\n",
       "      <td>1028.571429</td>\n",
       "      <td>82.742857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  x1  x2  x3  x4  x5  x6  x7  x8  x9   ...    x42  x43  x44  x45  x46  \\\n",
       "0   0   0   1   1   1   0   1   1   0   0   ...      1    1    5    2    2   \n",
       "1   1   0   1   1   1   0   1   1   0   0   ...      3    1    5    2    4   \n",
       "2   2   1   1   1   1   0   1   1   0   0   ...      1    8    3    1    4   \n",
       "3   3   1   1   1   0   0   1   1   0   0   ...      1    8    4    3    2   \n",
       "4   4   0   1   1   1   0   1   1   0   0   ...      1    7    5    2    4   \n",
       "\n",
       "        C_FP         C_FN        C_TP  C_TN  target  \n",
       "0  74.000000  1028.571429  121.828571     0       0  \n",
       "1  53.428571  1028.571429   82.742857     0       0  \n",
       "2  66.285714  1285.714286  102.928571     0       0  \n",
       "3  92.000000  1285.714286  151.785714     0       0  \n",
       "4  53.428571  1028.571429   82.742857     0       0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.952127\n",
       "1    0.047873\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X =data[['x'+str(i) for i in range(1, 47)]]\n",
    "y = data.target\n",
    "cost_mat = data[['C_FP','C_FN','C_TP','C_TN']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from  sklearn.cross_validation import train_test_split\n",
    "temp = train_test_split(X, y, cost_mat, test_size=0.33, random_state=42)\n",
    "X_train, X_test, y_train, y_test, cost_mat_train, cost_mat_test = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 20.1 (3 points)\n",
    "\n",
    "\n",
    "* Train 4 different models to predict target (Churn) using x1-x46 as features\n",
    "1. Logistic Regression\n",
    "2. Ensemble\n",
    "3. Under-sampling LR\n",
    "4. Under-sampling Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949289405685\n"
     ]
    }
   ],
   "source": [
    "# make predictions for testing set\n",
    "y_pred_class = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble - Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bagreg = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, \n",
    "                          bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24045069654234788"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under Sampling LR and Under Sampling Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def UnderSampling(X, y, target_percentage=0.5, seed=None):\n",
    "    # Assuming minority class is the positive\n",
    "    n_samples = y.shape[0]\n",
    "    n_samples_0 = (y == 0).sum()\n",
    "    n_samples_1 = (y == 1).sum()\n",
    "\n",
    "    n_samples_0_new =  n_samples_1 / target_percentage - n_samples_1\n",
    "    n_samples_0_new_per = n_samples_0_new / n_samples_0\n",
    "\n",
    "    filter_ = y == 0\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    rand_1 = np.random.binomial(n=1, p=n_samples_0_new_per, size=n_samples)\n",
    "    \n",
    "    filter_ = filter_ & rand_1\n",
    "    filter_ = filter_ | (y == 1)\n",
    "    filter_ = filter_.astype(bool)\n",
    "    \n",
    "    return X[filter_], y[filter_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {'lr': LinearRegression(),\n",
    "          'gb': BaggingClassifier()}\n",
    "\n",
    "X_u, y_u = UnderSampling(X_train, y_train,0.5,321)\n",
    "\n",
    "for model in models.keys():\n",
    "    models[model].fit(X_u, y_u)\n",
    "    \n",
    "# predict test for each model\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=models.keys())\n",
    "for model in models.keys():\n",
    "    y_pred[model] = models[model].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 20.2\n",
    "\n",
    "* Calculate the savings of the different models\n",
    "* Compare F1Score and Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947674418605\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb f1score 0.13821815154\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of continuous and binary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-aedc11c95383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    637\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    638\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    754\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 82\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of continuous and binary"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for model in models.keys():\n",
    "    print(model, 'f1score',metrics.f1_score(y_pred[model],y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 20.3 (2 points)\n",
    "\n",
    "Using the probabilities of each model estimate a BMR classifier\n",
    "\n",
    "Compare the savings and F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pre        f1       acc       rec       sav\n",
      "GB      0.060000  0.028986  0.935078  0.019108 -0.001947\n",
      "LR      0.000000  0.000000  0.949289  0.000000  0.000000\n",
      "GB-BMR  0.086188  0.146893  0.707364  0.496815  0.108048\n",
      "LR-BMR  0.083556  0.146646  0.646641  0.598726  0.138900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"GB\": {\"f\": BaggingClassifier()},\n",
    "               \"LR\": {\"f\": LogisticRegression()}}\n",
    "\n",
    "for model in classifiers.keys():\n",
    "    # Fit\n",
    "    classifiers[model][\"f\"].fit(X_train, y_train)\n",
    "    # Predict\n",
    "    classifiers[model][\"c\"] = classifiers[model][\"f\"].predict(X_test)\n",
    "    classifiers[model][\"p\"] = classifiers[model][\"f\"].predict_proba(X_test)\n",
    "    classifiers[model][\"p_train\"] = classifiers[model][\"f\"].predict_proba(X_train)\n",
    "    \n",
    "# Evaluate the performance\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "measures = {\"f1\": f1_score, \"pre\": precision_score, \n",
    "            \"rec\": recall_score, \"acc\": accuracy_score}\n",
    "results = pd.DataFrame(columns=measures.keys())\n",
    "\n",
    "# Evaluate each model in classifiers\n",
    "for model in classifiers.keys():\n",
    "    results.loc[model] = [measures[measure](y_test, classifiers[model][\"c\"]) for measure in measures.keys()]\n",
    "\n",
    "# Calculation of the cost and savings\n",
    "from costcla.metrics import savings_score, cost_loss \n",
    "\n",
    "# Evaluate the savings for each model\n",
    "results[\"sav\"] = np.zeros(results.shape[0])\n",
    "for model in classifiers.keys():\n",
    "    results[\"sav\"].loc[model] = savings_score(y_test, classifiers[model][\"c\"], cost_mat_test)\n",
    "\n",
    "\n",
    "\n",
    "from costcla.models import BayesMinimumRiskClassifier\n",
    "ci_models = list(classifiers.keys())\n",
    "\n",
    "for model in ci_models:\n",
    "    classifiers[model+\"-BMR\"] = {\"f\": BayesMinimumRiskClassifier()}\n",
    "    # Fit\n",
    "    classifiers[model+\"-BMR\"][\"f\"].fit(y_test, classifiers[model][\"p\"])  \n",
    "    # Calibration must be made in a validation set\n",
    "    # Predict\n",
    "    classifiers[model+\"-BMR\"][\"c\"] = classifiers[model+\"-BMR\"][\"f\"].predict(classifiers[model][\"p\"], cost_mat_test)\n",
    "    # Evaluate\n",
    "    results.loc[model+\"-BMR\"] = 0\n",
    "    results.loc[model+\"-BMR\", measures.keys()] = \\\n",
    "    [measures[measure](y_test, classifiers[model+\"-BMR\"][\"c\"]) for measure in measures.keys()]\n",
    "    results[\"sav\"].loc[model+\"-BMR\"] = savings_score(y_test, classifiers[model+\"-BMR\"][\"c\"], cost_mat_test)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAE8CAYAAADHfKdqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2g/owJ/IghLAjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108fd3320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "# Plot the results\n",
    "ind = np.arange(results.shape[0])\n",
    "figsize(10, 5)\n",
    "ax = plt.subplot(111)\n",
    "l = ax.plot(ind, results[\"f1\"], \"-o\", label='F1Score')\n",
    "b = ax.bar(ind-0.3, results['sav'], 0.6, label='Savings')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlim([-0.5, ind[-1]+.5])\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(results.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logistica no da buenos resultados ni con el F1Score ni con Saving. Podemos ver que al aplicar BMR ambos metodos obtienen mejores resultados pero sobresale la regresión logistica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 20.4\n",
    "\n",
    "Estimate a CostSensitiveDecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d7cd360ebd88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CSDT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCostSensitiveDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CSDT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_mat_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CSDT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CSDT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/costcla/models/cost_tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cost_mat, check_input)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree_grow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/costcla/models/cost_tree.py\u001b[0m in \u001b[0;36m_tree_grow\u001b[0;34m(self, y_true, X, cost_mat, level)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# Calculate the best split of the current node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXl_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/costcla/models/cost_tree.py\u001b[0m in \u001b[0;36m_best_split\u001b[0;34m(self, y_true, X, cost_mat)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# For each feature test all possible splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/FelipeGarcia/anaconda/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1087\u001b[0m         \u001b[0;34m\"\"\" return the cached item, item represents a label indexer \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "from costcla.models import CostSensitiveDecisionTreeClassifier\n",
    "\n",
    "classifiers[\"CSDT\"] = {\"f\": CostSensitiveDecisionTreeClassifier()}\n",
    "# Fit\n",
    "classifiers[\"CSDT\"][\"f\"].fit(X_train, y_train, cost_mat_train)\n",
    "# Predict\n",
    "classifiers[\"CSDT\"][\"c\"] = classifiers[\"CSDT\"][\"f\"].predict(X_test)\n",
    "# Evaluate\n",
    "results.loc[\"CSDT\"] = 0\n",
    "results.loc[\"CSDT\", measures.keys()] = \\\n",
    "[measures[measure](y_test, classifiers[\"CSDT\"][\"c\"]) for measure in measures.keys()]\n",
    "results[\"sav\"].loc[\"CSDT\"] = savings_score(y_test, classifiers[\"CSDT\"][\"c\"], cost_mat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
